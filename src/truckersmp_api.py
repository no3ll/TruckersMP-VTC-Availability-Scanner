import json
from pathlib import Path
from typing import List

import requests
from bs4 import BeautifulSoup

# ---------- CONSTANTS ----------

BASE_SITE_URL = "https://truckersmp.com"
PROJECT_ROOT = Path(__file__).resolve().parent.parent
VTC_SOURCE_FILE = (PROJECT_ROOT / "vtcs_source.json").resolve()

USER_AGENT = "TCCR-VTC-Finder/0.1 (contact: your-discord-or-email)"  # customize this


# ---------- EVENT SCRAPING: WHICH VTCs ARE BUSY ----------

def _fetch_event_page_html(event_id: int) -> str:
    """
    Download the event HTML page from truckersmp.com.
    """
    url = f"{BASE_SITE_URL}/events/{event_id}"
    print(f"[DEBUG] Requesting event page: {url}")
    resp = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=10)
    resp.raise_for_status()
    return resp.text


def _parse_attending_vtc_ids(html: str) -> List[int]:
    """
    Parse attending VTC IDs from the event HTML.

    Strategy:
      - Parse HTML with BeautifulSoup
      - Find <a href="..."> links containing "/vtc/"
      - Extract numeric part right after /vtc/
    """
    soup = BeautifulSoup(html, "html.parser")
    vtc_ids: set[int] = set()

    for a in soup.find_all("a", href=True):
        href = str(a.get("href", ""))

        if "/vtc/" not in href:
            continue

        try:
            after = href.split("/vtc/")[1]  # "12345" or "12345-name"
            id_str = ""
            for ch in after:
                if ch.isdigit():
                    id_str += ch
                else:
                    break
            if id_str:
                vtc_ids.add(int(id_str))
        except (IndexError, ValueError):
            continue

    return sorted(vtc_ids)


def get_attending_vtcs(event_id: int) -> List[int]:
    """
    Public function: get attending VTC IDs for a given event by scraping TMP.
    """
    print(f"[DEBUG] Fetching real attending VTCs for event {event_id}...")
    html = _fetch_event_page_html(event_id)
    ids = _parse_attending_vtc_ids(html)
    print(f"[DEBUG] Found {len(ids)} VTC(s) linked on the event page.")
    return ids


# ---------- AUTO-DISCOVER EVENTS FOR A DATE ----------

def find_events_for_date(date_str: str, max_pages: int = 5) -> List[str]:
    """
    Try to discover public TruckersMP events for a specific date.

    It hits pages like:
      https://truckersmp.com/events?date=YYYY-MM-DD&page=1

    and extracts links of the form /events/<id>-something.

    Returns a list of full event URLs.
    """
    print(f"[INFO] Searching events for date {date_str}...")
    seen_ids: set[int] = set()
    urls: list[str] = []

    for page in range(1, max_pages + 1):
        list_url = f"{BASE_SITE_URL}/events?date={date_str}&page={page}"
        print(f"[DEBUG] Requesting events list page: {list_url}")

        try:
            resp = requests.get(list_url, headers={"User-Agent": USER_AGENT}, timeout=10)
        except Exception as e:
            print(f"[WARN] Request failed for events list page {page}: {e}")
            break

        if resp.status_code != 200:
            print(f"[WARN] Got HTTP {resp.status_code} for events list page {page}, stopping.")
            break

        soup = BeautifulSoup(resp.text, "html.parser")
        found_on_page = False

        for a in soup.find_all("a", href=True):
            href = str(a.get("href", ""))

            if not href.startswith("/events/"):
                continue

            # Avoid links that aren't actual event detail pages, just to be safe
            try:
                after = href.split("/events/")[1]
                id_str = ""
                for ch in after:
                    if ch.isdigit():
                        id_str += ch
                    else:
                        break
                if not id_str:
                    continue

                event_id = int(id_str)
                if event_id not in seen_ids:
                    seen_ids.add(event_id)
                    full_url = f"{BASE_SITE_URL}/events/{event_id}"
                    urls.append(full_url)
                    found_on_page = True
            except Exception:
                continue

        if not found_on_page:
            # No more events for this date, probably
            break

    print(f"[INFO] Found {len(urls)} event(s) for {date_str}.")
    return urls


# ---------- CANDIDATE VTC LIST FROM vtcs_source.json ----------

def load_candidate_vtcs() -> list[dict]:
    """
    Load candidate VTCs from vtcs_source.json.
    This file is generated by generate_vtcs_json.py and optionally augmented by augment_vtcs_from_events.py.
    """
    if not VTC_SOURCE_FILE.exists():
        raise FileNotFoundError(
            f"{VTC_SOURCE_FILE} not found. Run generate_vtcs_json.py first."
        )

    with VTC_SOURCE_FILE.open("r", encoding="utf-8") as f:
        data = json.load(f)

    cleaned: list[dict] = []
    for v in data:
        try:
            cleaned.append(
                {
                    "id": int(v["id"]),
                    "name": str(v.get("name", f"VTC {v['id']}")),
                    "members": int(v.get("members", 0)),
                    "status": str(v.get("status", "normal")),
                }
            )
        except (KeyError, ValueError, TypeError):
            # Skip invalid entries quietly for now
            continue

    print(f"[DEBUG] Loaded {len(cleaned)} candidate VTC(s) from vtcs_source.json.")
    return cleaned


if __name__ == "__main__":
    print(f"PROJECT_ROOT = {PROJECT_ROOT}")
    print(f"VTC_SOURCE_FILE = {VTC_SOURCE_FILE}")
    try:
        vtcs = load_candidate_vtcs()
        print(f"Loaded {len(vtcs)} VTC(s):")
        for v in vtcs[:10]:
            print(" ", v)
    except FileNotFoundError as e:
        print("Error:", e)
